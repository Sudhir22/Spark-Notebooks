{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS246 - Colab 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sudhir22/Spark-Notebooks/blob/main/MLlib_frequent_itemsets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-qHai2252mI"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CJ71AKe91eh"
      },
      "source": [
        "Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K93ABEy9Zlo"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0orRvrc1-545"
      },
      "source": [
        "id='1dhi1F78ssqR8gE6U-AgB80ZW7V_9snX4'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('products.csv')\n",
        "\n",
        "id='1KZBNEaIyMTcsRV817us6uLZgm-Mii8oU'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('order_products__train.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ"
      },
      "source": [
        "If you executed the cells above, you should be able to see the dataset we will need for this Colab under the \"Files\" tab on the left panel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twk-K-jilWK7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.ml.fpm import FPGrowth"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr-8fK-1lmY0"
      },
      "source": [
        "Let's initialize the Spark context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOwtm2l7lePt"
      },
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-LpyCLzlul6"
      },
      "source": [
        "You can easily check the current version and get the link of the web interface. In the Spark UI, you can monitor the progress of your job and debug the performance bottlenecks (if your Colab is running with a **local runtime**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g87iz4klwYJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "de5642cd-a428-43eb-e457-a4605ad68ca2"
      },
      "source": [
        "spark"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a92bfa0117e0:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f1d7d112a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaF2A_j_nC7"
      },
      "source": [
        "### Your task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebLNUxP0_8x3"
      },
      "source": [
        "If you run successfully the setup stage, you are ready to work with the **3 Million Instacart Orders** dataset. In case you want to read more about it, check the [official Instacart blog post](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2) about it, a concise [schema description](https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b) of the dataset, and the [download page](https://www.instacart.com/datasets/grocery-shopping-2017).\n",
        "\n",
        "In this Colab, we will be working only with a small training dataset (~131K orders) to perform fast Frequent Pattern Mining with the FP-Growth algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu-e7Ph2_ruG"
      },
      "source": [
        "products = spark.read.csv('products.csv', header=True, inferSchema=True)\n",
        "orders = spark.read.csv('order_products__train.csv', header=True, inferSchema=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhxZZRT9syUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88206aa6-c928-4962-b825-ffa852f7beab"
      },
      "source": [
        "products.printSchema()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- product_name: string (nullable = true)\n",
            " |-- aisle_id: string (nullable = true)\n",
            " |-- department_id: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VeRYRz2s1pm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a48b47-1f48-4404-9095-2e7ad047d13c"
      },
      "source": [
        "orders.printSchema()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- add_to_cart_order: integer (nullable = true)\n",
            " |-- reordered: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5muD_Io59CG"
      },
      "source": [
        "Using the Spark Dataframe API to join 'products' and 'orders', so that we will be able to see the product names in each transaction (and not only their ids).  Then, grouping by the orders by 'order_id' to obtain one row per basket (i.e., set of products purchased together by one customer). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRH4o4p7s7V6"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "joined_data = orders.join(products,on=['product_id'],how='left')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq1Nmi_i1tsr",
        "outputId": "6af8b2e1-ab68-451b-8e21-a8c874c77b92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grouped_data = joined_data.groupby(['order_id']).agg(collect_set('product_name').alias(\"product_set\"))\n",
        "grouped_data.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+\n",
            "|order_id|         product_set|\n",
            "+--------+--------------------+\n",
            "|    1342|[Raw Shrimp, Seed...|\n",
            "|    1591|[Cracked Wheat, S...|\n",
            "|    4519|[Beet Apple Carro...|\n",
            "|    4935|             [Vodka]|\n",
            "|    6357|[Globe Eggplant, ...|\n",
            "|   10362|[Organic Baby Spi...|\n",
            "|   19204|[Reduced Fat Crac...|\n",
            "|   29601|[Organic Red Onio...|\n",
            "|   31035|[Organic Cripps P...|\n",
            "|   40011|[Organic Baby Spi...|\n",
            "|   46266|[Uncured Beef Hot...|\n",
            "|   51607|[Donut House Choc...|\n",
            "|   58797|[Concentrated But...|\n",
            "|   61793|[Raspberries, Gre...|\n",
            "|   67089|[Original Tofurky...|\n",
            "|   70863|[Extra Hold Non-A...|\n",
            "|   88674|[Organic Coconut ...|\n",
            "|   91937|[No. 485 Gin, Mon...|\n",
            "|   92317|[Red Vine Tomato,...|\n",
            "|   99621|[Organic Baby Aru...|\n",
            "+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfHoTLAg6qnM"
      },
      "source": [
        "In this Colab we will explore [MLlib](https://spark.apache.org/mllib/), Apache Spark's scalable machine learning library. Specifically, you can use its implementation of the [FP-Growth](https://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html#fp-growth) algorithm to perform efficiently Frequent Pattern Mining in Spark.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boWgxXNns089"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "fpGrowth = FPGrowth(itemsCol=\"product_set\",minSupport=0.01,minConfidence=0.5)\n",
        "model = fpGrowth.fit(grouped_data)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kpTVdfD8UiO"
      },
      "source": [
        "Compute how many frequent itemsets and association rules were generated by running FP-growth.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KYgQ_URunvA"
      },
      "source": [
        "model.freqItemsets.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhsdU_2oDVpe"
      },
      "source": [
        "model.associationRules.count()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}